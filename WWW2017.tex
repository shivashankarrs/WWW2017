% This is "sig-alternate.tex" V2.1 April 2013
% This file should be compiled with V2.5 of "sig-alternate.cls" May 2012
%
% This example file demonstrates the use of the 'sig-alternate.cls'
% V2.5 LaTeX2e document class file. It is for those submitting
% articles to ACM Conference Proceedings WHO DO NOT WISH TO
% STRICTLY ADHERE TO THE SIGS (PUBS-BOARD-ENDORSED) STYLE.
% The 'sig-alternate.cls' file will produce a similar-looking,
% albeit, 'tighter' paper resulting in, invariably, fewer pages.
%
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V2.5) produces:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) NO page numbers
%
% as against the acm_proc_article-sp.cls file which
% DOES NOT produce 1) thru' 3) above.
%
% Using 'sig-alternate.cls' you have control, however, from within
% the source .tex file, over both the CopyrightYear
% (defaulted to 200X) and the ACM Copyright Data
% (defaulted to X-XXXXX-XX-X/XX/XX).
% e.g.
% \CopyrightYear{2007} will cause 2007 to appear in the copyright line.
% \crdata{0-12345-67-8/90/12} will cause 0-12345-67-8/90/12 to appear in the copyright line.
%
% ---------------------------------------------------------------------------------------------------------------
% This .tex source is an example which *does* use
% the .bib file (from which the .bbl file % is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission, you *NEED* to 'insert'
% your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% ================= IF YOU HAVE QUESTIONS =======================
% Questions regarding the SIGS styles, SIGS policies and
% procedures, Conferences etc. should be sent to
% Adrienne Griscti (griscti@acm.org)
%
% Technical questions _only_ to
% Gerald Murray (murray@hq.acm.org)
% ===============================================================
%
% For tracking purposes - this is V2.0 - May 2012


\documentclass[letterpaper]{sig-alternate-2013}

\usepackage{floatrow}
\usepackage{url}
\usepackage{array}
\usepackage{amsmath}
\usepackage{authblk,xspace,paralist,booktabs}
\usepackage[pdfborder={0 0 0}]{hyperref}% For email addresses

\permission{\copyright 2017 International World Wide Web Conference Committee (IW3C2), 
published under Creative Commons CC BY 4.0 License.}
\conferenceinfo{WWW 2017,}{April 3--7, 2017, Perth, Australia.}
\copyrightetc{ACM \the\acmcopyr}
\crdata{978-1-4503-4914-7/17/04. \\
http://dx.doi.org/10.1145/3041021.3054224  \\
\includegraphics{cc.png}
%<img src="http://i.creativecommons.org/l/by/4.0/88x31.png" /> 
}
 
\clubpenalty=10000
\widowpenalty = 10000

\def\@maketitle{\newpage
 \null
 \setbox\@acmtitlebox\vbox{%
\baselineskip 20pt
\vskip 2em                   % Vertical space above title.
   \begin{center}
    {\ttlfnt \@title\par}       % Title set in 18pt Helvetica (Arial) bold size.
    \vskip 1.5em                % Vertical space after title.
%This should be the subtitle.
{\subttlfnt \the\subtitletext\par}\vskip 1.25em%\fi
    {\baselineskip 16pt\aufnt   % each author set in \12 pt Arial, in a
     \lineskip .5em             % tabular environment
     \begin{tabular}[t]{c}\@author
     \end{tabular}\par}
    \vskip 1.5em               % Vertical space after author.
   \end{center}}
 \dimen0=\ht\@acmtitlebox
% \advance\dimen0 by -12.75pc\relax % comment by Marco Daniel
 \unvbox\@acmtitlebox
 \ifdim\dimen0<0.0pt\relax\vskip-\dimen0\fi}



\newcommand{\query}[1]{\texttt{#1}\xspace}
\newcommand{\ex}[1]{\textit{#1}\xspace}
\newcommand{\tabref}[1]{Table~\ref{#1}\xspace}
\newcommand{\secref}[2][]{Section#1~\ref{#2}\xspace}


\title{Pairwise Webpage Coreference Classification Using Distant Supervision\vspace{-0.7cm}}

\author[1*]{\Large{S. Shivashankar}}
\author[2*]{\Large{Timothy Baldwin}}
\author[3*]{\Large{Julian Brooke}}
\author[4*]{\Large{Trevor Cohn}\vspace{-0.3cm}}
\affil[*]{\large Computing and Information Systems, The University
  of Melbourne, Australia}
\affil[1]{\url{shivashankar@student.unimelb.edu.au}} 
\affil[2,3,4] {\url{{tbaldwin,julian.brooke,t.cohn}@unimelb.edu.au}}

\hyphenation{spec-ific-ally}

\begin{document}


% Copyright
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}


% DOI
%\doi{10.475/123_4}

% ISBN
%\isbn{123-4567-24-567/08/06}

%Conference
%\conferenceinfo{PLDI '13}{June 16--19, 2013, Seattle, WA, USA}

%\acmPrice{\$15.00}

%
% --- Author Metadata here ---
%\conferenceinfo{WOODSTOCK}{'97 El Paso, Texas USA}
%\CopyrightYear{2007} % Allows default copyright year (20XX) to be over-ridden - IF NEED BE.
%\crdata{0-12345-67-8/90/01}  % Allows default copyright data (0-89791-88-6/97/05) to be over-ridden - IF NEED BE.
% --- End of Author Metadata ---

%\title{Pairwise Web URL classification for Cross-KB Coreference Resolution using Distant Supervision}
%\title{Pairwise Web URL classification for Cross-KB Entity Linking using Distant Supervision}
%\title{Pairwise Webpage Coreference Classification using Distant Supervision}
%
% You need the command \numberofauthors to handle the 'placement
% and alignment' of the authors beneath the title.
%
% For aesthetic reasons, we recommend 'three authors at a time'
% i.e. three 'name/affiliation blocks' be placed beneath the title.
%
% NOTE: You are NOT restricted in how many 'rows' of
% "name/affiliations" may appear. We just ask that you restrict
% the number of 'columns' to three.
%
% Because of the available 'opening page real-estate'
% we ask you to refrain from putting more than six authors
% (two rows with three columns) beneath the article title.
% More than six makes the first-page appear very cluttered indeed.
%
% Use the \alignauthor commands to handle the names
% and affiliations for an 'aesthetic maximum' of six authors.
% Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the
% \additionalauthors command.
% These 'additional authors' will be output/set for you
% without further effort on your part as the last section in
% the body of your article BEFORE References or any Appendices.


%\numberofauthors{4}
%\author{
%      \alignauthor S. Shivashankar\\      
%      \affaddr{Department of CIS,}\\
%      \affaddr{The University of Melbourne}\\
%      \small{shivashankar@student.unimelb.edu.au}
%%
%      \alignauthor Tim Baldwin\\    
%      \affaddr{Department of CIS,}\\
%      \affaddr{The University of Melbourne}\\
%	\small{tb@ldwin.net}\\
%%
%      \alignauthor Julian Brooke\\    
%      \affaddr{Department of CIS,}\\
%      \affaddr{The University of Melbourne}\\
%\small{julian.brooke@unimelb.edu.au}
%\and
%      \alignauthor Trevor Cohn\\     
%      \affaddr{Department of CIS,}\\
%      \affaddr{The University of Melbourne}\\
%      \small{t.cohn@unimelb.edu.au}
%}

%\numberofauthors{4} %  in this sample file, there are a *total*
%% of EIGHT authors. SIX appear on the 'first-page' (for formatting
%% reasons) and the remaining two appear in the \additionalauthors section.
%%
%\author{
%% You can go ahead and credit any number of authors here,
%% e.g. one 'row of three' or two rows (consisting of one row of three
%% and a second row of one, two or three).
%%
%% The command \alignauthor (no curly braces needed) should
%% precede each author name, affiliation/snail-mail address and
%% e-mail address. Additionally, tag each line of
%% affiliation/address with \affaddr, and tag the
%% e-mail address with \email.
%%
%% 1st. author
%\alignauthor S. Shivashankar\\
%%  \affaddr{Department of CIS,}\\
% %  S\affaddr{University of Melbourne}\\
%       \email{shivashankar@student.unimelb.edu.au}
%% 2nd. author
%\alignauthor
%Trevor Cohn\\
% %   \affaddr{Department of CIS,}\\
%   %    \affaddr{University of Melbourne}\\
%	\small {t.cohn@unimelb.edu.au}
%% 3rd. author
%\alignauthor Tim Baldwin\\
%  %     \affaddr{Department of CIS,}\\
%   %    \affaddr{University of Melbourne}\\
%       \small{tb@ldwin.net}
%\alignauthor Julian Brooke\\
%  %     \affaddr{Department of CIS,}\\
%  %     \affaddr{University of Melbourne}\\
%       \small{julian.brooke@gmail.com}
%
%   \sharedaffiliation
%      \affaddr{Department of Computing and Information System}  \\
%      \affaddr{The University of Melbourne}   \\
%      \affaddr{VIC, Australia}
%}

%To achieve a good coverage of entities across multiple domains, web URL end-points can be considered as a knowledge base (KB). Since 
\maketitle
\begin{abstract}
%Our objective is to determine whether two candidate endpoint URLs refer to the same underlying entity. 
  A person or other entity is often associated with multiple URL endpoints on the web, motivating the task of determining whether a given pair of webpages is coreferent to a given entity. To strike a balance between unsupervised and supervised methods that require annotated data, we build a positive and unlabelled (PU) learning model, where we obtain positive examples using web search-based distant supervision.  We evaluate our proposed approach using the SemEval-2007 WePS and ALTA-2016 shared task datasets.

%Since web URL end-points consist of entries from multiple KBs, we address a fundamental sub-task of cross-KB coreference resolution. 


%This can be seen as a precursor to grouping web endpoints into coreferent sets which can enable various tasks such as jointly disambiguating entities in target %text, entity related information extraction, etc. 

%This pairwise version of the task serves 
%as an important precursor to the general problem
%of clustering web endpoints into coreferent sets.

%In this work we treat any valid web URI as an end-point that can be used to disambiguate an entity.

%For mapping entities in given text to web URI end-points (web KB), we must first infer its existence on the web. We refer to this task as Knowledge Base %Discovery (KBD). For every entity endpoint we may recover thousands of entity mentions via inlinks. While the effectiveness of inlink-driven entity disambiguation %is known for a single KB setting, this can be extended to leverage inlinks across a collection of automatically discovered web KBs. We address the problem of %pair-wise URI classification based entity disambiguation with  distant supervision, by leveraging bootstrapped positive examples using web search and unlabelled %data.  We empirically evaluate our proposed approach using SemEval-2007 WePS and ALTA-2016 shared task datasets.

\end{abstract}


%
% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
% Please copy and paste the code instead of the example below. 
%
%\begin{CCSXML}
%<ccs2012>
% <concept>
%  <concept_id>10010520.10010553.10010562</concept_id>
%  <concept_desc>Computer systems organization~Embedded systems</concept_desc>
%  <concept_significance>500</concept_significance>
% </concept>
% <concept>
%  <concept_id>10010520.10010575.10010755</concept_id>
%  <concept_desc>Computer systems organization~Redundancy</concept_desc>
%  <concept_significance>300</concept_significance>
% </concept>
% <concept>
%  <concept_id>10010520.10010553.10010554</concept_id>
%  <concept_desc>Computer systems organization~Robotics</concept_desc>
%  <concept_significance>100</concept_significance>
% </concept>
% <concept>
%  <concept_id>10003033.10003083.10003095</concept_id>
%  <concept_desc>Networks~Network reliability</concept_desc>
%  <concept_significance>100</concept_significance>
% </concept>
%</ccs2012>  
%\end{CCSXML}

%\ccsdesc[500]{Computer systems organization~Embedded systems}
%\ccsdesc[300]{Computer systems organization~Redundancy}
%\ccsdesc{Computer systems organization~Robotics}
%\ccsdesc[100]{Networks~Network reliability}


%
% End generated code
%

%
%  Use this command to print the description
%
%\printccsdesc

% We no longer use \terms command
%\terms{Theory}

\keywords{Webpage Coreference; End-point Disambiguation; Distant Supervision; PU Learning; Semi-supervised Learning}

\section{Introduction}

%resolves textual mentions to the correct node in a knowledge base (KB). Linking 
%For example, the URL \url{en.wikipedia.org/wiki/Barack_Obama} may be used in reference to former US president Barack Obama. Inlinks to this page are unlikely to refer to some other entity, so we consider this a disambiguating endpoint.

Entity endpoints are URLs which reliably disambiguate named entity mentions on the web \cite{chisholm2016akbc}. A target entity can be a person, organisation, location, event or concept. Entity linking systems typically rely on semantic resources such as Wikipedia or DBPedia as end-points. Though they provide rich context for entities, such sources do not have high coverage for many domains. In addition to traditional sources, fortunately many other end-points exist for an entity on the web: for instance,  social networks (e.g.\ \url{facebook.com/*}), news aggregation endpoints (e.g.\ \url{nytimes.com/topic/person/*}) and organisation directories (e.g.\ \url{gtlaw.com/People/*}) are largely untapped  sources of valuable entity information. Each source can be treated as a knowledge base (KB) containing entity-related unstructured data (webpages) identified by URL end-points.\footnote{``URL'' henceforth is used to refer to URL end-points.} Building on automatic approaches for discovering web KBs \cite{chisholm2016akbc}, we focus on the downstream challenge of end-point reconciliation across KBs. Specifically, similar to the ALTA 2016 shared task,\footnote{\url{http://alta.asn.au/events/sharedtask2016/}} given two candidate endpoint URLs, our task is to determine whether they refer to the same underlying entity. This can be seen as a precursor to grouping web endpoints into coreferent sets. This relates to work on web person search (WePS), i.e.\ the unsupervised task of clustering entity mention pages instead of entity endpoint pages, i.e., the pages can have content related to multiple entities.
%, where the overall goal is also to cluster web pages
%Similar to ALTA 2016 shared task\footnote{http://alta.asn.au/events/sharedtask2016/description.html}, we address the problem of Cross-KB coreference resolution. 

%Web links can produce models nearly as accurate as those built from richly structured KBs .
%
%
%While Wikipedia has been used extensively for automated entity recognition and disambiguation, many other endpoints may exist for an entity on the web. For example, \url{nytimes.com/topic/person/barack-obama} and \url{twitter.com/BarackObama} may be used equivalently. This style of systematic entity indexing is characteristic
%of social sources (e.g. facebook.com/* ), news aggregation endpoints (e.g. \url{nytimes.com/topic/person/*} ) and organisation directories (e.g. \url{gtlaw.com/People/*} ). These resources present a valuable and largely untapped source of entity information, both in the content they host and semantic resources that may be extracted from inbound links.
%
%Entity linking (EL) systems typically rely on semantic resources such as Wikipedia, DBPedia to be used as end-points. Though they provide a rich context for entities, these sources do not achieve sufficient recall for all the domains and entities. On the other hand, domain centered sources such as DBLP, IMDb or MusicBrainz would have the depth in coverage for a target domain only. 
%%It is understandable that a near complete solution would depend on merging resources from multiple knowledge bases (KBs). 
%%But an explicit reconciliation of distinct entity sets and heterogeneous sources' schema is a challenging problem in itself \cite{chisholm2016akbc}. 
% We consider the sub-task of KBD on web which requires URI disambiguation based on in-links for entity linking. For example, we may observe that inlinks to \url{en.wikipedia.org/wiki/Barack_Obama} are typically mentions of the entity Barack Obama. Inlink based disambiguation can be handled as a pair-wise URI classification problem. URI Pairs are classified as positive if they refer to the same entity and negative otherwise. 


% (b) TREC relevance feedback track\footnote{http://trec.nist.gov/data/relevance.feedback.html} had tasks related to relevance classification for a set of 
% documents given a query. We can treat them as similar problems when solved as pair-wise binary document classification. 



% This solution is motivated by \cite{hachenbergfinding}, where relation labels from KB are used as queries. Here we use named entities from webpage content as % context keywords to query.  In \cite{hachenbergfinding}, the reverse task was addressed, to map entities in a KB to web URL end-points. 
% This also helps in building a weakly supervised model for tuning parameters in WePS systems.
% To ease the constraint of requiring annotated data for this task, as done in state-of-art approaches \cite{chisholm2015entity}, we propose to leverage web-search-based distant supervision for acquiring labels automatically. 
% 
Rather than building a completely unsupervised model \cite{chisholm2016akbc, delgado2014data, nuray2009exploiting} or a fully supervised model, as in the ALTA shared task, we propose an approach to generate distantly supervised positive examples using web-search, and employ a positive and unlabelled (PU) learning algorithm \cite{Elkan:2008:LCO:1401890.1401920, liu2002partially}. The intuition underlying our method is that, if we query for a target entity name $e$ with the addition of a disambiguating keyword extracted from URL$_{a}$, and URL$_{b}$ appears in the top search results, then URL$_a$ and URL$_b$ are highly likely to refer to the same entity $e$ (i.e.\ be coreferent). If, on the other hand, URL$_{b}$ is not included in the top search results, it is not possible to draw any conclusion as to whether the two URLs are coreferent or not. For example, while Wikipedia and \url{biography.com} both contain URLs for singer George Clinton, denoted $W_1$ and $B_1$, respectively, the search query \query{"George Clinton" AND P-Funk} --- where \query{P-Funk} is a disambiguating term extracted from $W_1$ --- does not list $B_1$ as one of the top-ranked results. 
%Analogous to \cite{hachenbergfinding}, we use web search for URL pair classification.

\section{Proposed Approach}
% We assume to have a web URL end-points KB built using automatic approaches like  \cite{chisholm2016akbc}. From such KB, given a training dataset $D$ with pairs of web URLs, our objective is to learn a pair-wise URL classifier. For a URL pair $U_{1}$ and $U_{2}$, we learn a model \begin{small} $\quad f(\phi(U_{1}, U_{2})) \to y$ \end{small} where the target $y \in \{0,1\}$ denotes whether the URL pair refers to the same entity ($y=1$) or not ($y=0$).  $\phi$ is the feature map which is used to generate pair-wise URL document features, which is denoted as x. Initially all the pairs are unlabelled $D_{u}$=$D$, positive and negative labeled sets are empty $D_{p}, D_{n}$ = $\emptyset$.

We assume access to web KBs based on automatic crawling \cite{chisholm2016akbc}. 
From such KBs, given a training dataset $D$ with web URL pairs from different KBs that refer to the same entity name
 (e.g.\ \ex{Michael Jordan}) but are not necessarily coreferent to the same entity, our objective is to learn a pairwise URL classifier. For a URL pair $U_{i}$ and $U_{j}$, we learn a model \begin{small} $\quad f(x) \to y$ \end{small} where the target $y \in \{0,1\}$ denotes whether the URL pair refers to the same entity ($y=1$) or not ($y=0$).  $x$ is a feature map for generating pairwise URL document features ($x=\phi(U_{i}, U_{j}$)). Initially, all pairs are unlabelled ($D_{u}=D$), and the positive and negative labelled sets are empty ($D_{p}, D_{n} = \emptyset$).

%Since acquiring labeled data for this task would require huge resources,  we propose to leverage web search results for providing distant supervision labels. 
\subsection{Distant Supervision} 
\label{sec:querying}

We construct web search queries for distant supervision as follows:
%\begin{itemize}
%\item 
\textbf{$Q_i$:} Using the target entity name and context information from $U_{i}$, in the form of person name (PER) and organisation (ORG) named entity instance extracted from $U_i$ by the Stanford CoreNLP NER toolkit.  Similar to \cite{nuray2009exploiting}, this takes the form of each of the first 8 NEs occurring in the title and document body (for a total of up to 8 separate queries), based on the intuition that the NEs that are most relevant to the target entity will be mentioned earlier in the document.
%\item 
\textbf{$Q_j$:} Similar to the above, but we generate the context information from  $U_{j}$.
%\end{itemize}

All of our experiments are based on the DuckDuckGo search engine. For each query in $Q_i$, we check to see if $U_{j}$ is present in the top-$K$ search results (we use $K$ = 30), and conversely if $U_i$ is present in the top-$K$ results for each query in $Q_j$. If the given URL is found in the result set for at least one of these (up to 16) queries, we assign the distant label $\hat{y_{ij}}=1$ to $U_{i}$ and $U_{j}$. These positive instances are added to $D_{p}$ and removed from $D_{u}$.

%If H is the total number of hits, distant label  $\hat{y_{ij}}$ for $URL_{i}$ and $URL_{j}$ is given as $\hat{y_{ij}} = 1, \text{if } H \ge \theta $. We use a very simple setup of $\theta$=1 in our experiments. 
%\begin{small}
%\[
%    \hat{y_{ij}} = 
%\begin{cases}
%    1,             &\text{if } H \ge \theta \\
%    0,              & \text{otherwise}
%\end{cases}
%\]
%\end{small}

%a binary classifier(with $D_{p}$ and $D_{u}$ as two classes)

% We propose a variant of \cite{Elkan:2008:LCO:1401890.1401920} to handle this setup.
% \begin{small} $\frac{1}{|D_{pc}|}\sum_{i:D_{pc}} g(x_{i})$ \end{small} for a randomly chosen subset of positively labeled examples $D_{pc}$, 

\subsection{Labelling Unlabelled Pairs} 
\label{sec:labelling}

Since the proportion of positive examples will typically be small, we
supplement $D_p$ as follows:
\begin{compactitem}
\item Step 1: Randomly select $N$ instances from $D_{p}$, and hold them out in $S_{p}$.
\item Step 2: Train a binary classifier $\theta$, taking $D'_{p} = D_p \setminus S_p$ as positive instances and $D_{u}$ as negative instances. We use a linear-kernel SVM classifier in our experiments.
\item Step 3: Compute the average score of instances in $S_{p}$ assigned by $\theta$, \mbox{$\mu_{p} = \frac{1}{|S_{p}|}\sum_{i:S_{p}} p(x_{i}=1)$,} using Platt scaling for probabilistic scoring. Form the set $D_p^*$ of instances in $D_{u}$ where the $\theta$-assigned score exceeds this threshold, i.e.\ $D_p^* = {x_u \in D_u: p(x_{u}=1) > \mu_{p}}$.
\end{compactitem}

This is a variant of the approaches of \cite{Elkan:2008:LCO:1401890.1401920} and \cite{liu2002partially}, and results in a training set of positive instances ($D_{p} \cup D^*_{p}$) and negative instances ($D_{u} \setminus D^*_{p}$), over which any standard binary classifier can be trained. In our experiments, we use a linear kernel SVM, which we refer to as ``DP-SVM'' in \tabref{tab:results}, since it uses propagated distant labels.
% TJB: cleaned up the notation variously; @Shiva to check the details

%The updated threshold for classifiying examples in $D_{u}$ is given below
%We use 
%
%We use one-class SVM to model positive examples obtained from distant supervision. We can write the distance to hyperplane using one-class SVM as
%\begin{small}
%\begin{align}
%g(x) & = \sum_{i:S_{p}} \alpha_i K(x,x_i) - \rho
%\end{align}
%\end{small}
%$S_{p}$ is the set of support vectors obtained using positively labeled examples. K is any kernel of choice and $\rho$ is the offset. The proportion of positive examples may not be large in all cases. Rather than using average decision value of positive examples obtained using a binary classifier(with $D_{p}$ and $D_{u}$ as two classes) to scale the test data finally as in \cite{Elkan:2008:LCO:1401890.1401920}, we use \begin{small} $\frac{1}{|D_{pc}|}\sum_{i:D_{pc}} g(x_{i})$ \end{small} for a randomly chosen subset of positively labeled examples $D_{pc}$, referred as $\mu_{p}$ to provide a new conservative margin for labeling data-points in $D_{u}$. The updated threshold for classifying examples in $D_{u}$ is given below
%
%%We treat the SVM score of a data-point in $D_{u}$ using the decision function above as its closeness to the region of positive examples, then we consider %the robust measure $\frac{1}{|D_{p}|}\sum_{i:D_{p}} g(x_{i})$, referred as $\mu_{p}$, as the threshold for classifying examples in $D_{u}$ as positive or %negative. 
%\begin{small}
%\begin{align}
%f(x) & = g(x) - \mu_{p}, \forall x \in D_{u} 
%\end{align}
%
%\[
%    \hat{y(x)} = 
%\begin{cases}
%    1,             &\text{if } f(x) > 0    \\
%    0,              & \text{otherwise}
%\end{cases}
%\]
%\end{small}
%
%The objective of this step is to classify the unlabelled/ambiguous examples ($D_{u}$) as positive or negative based on its distance to the region of positive examples ($D_{p}$), the labeled set is denoted as $D^l_{u}$. With this we would have a dataset with positive and negative examples ($D_{p} \cup D^l_{u}$) which can be used with any standard binary classifier.
%
\section{Experiments}

We present results over the SemEval-2007 WePS\footnote{\url{http://nlp.uned.es/weps/weps-1}} development set and ALTA-2016 shared task datasets. For WePS, we constructed a balanced 1000 URL-pair dataset from webpages for 49 people. We only included pairs sharing the same person name from webpages that still exist on the web. Additionally, we considered end-point pages only (filtered using features from \cite{chisholm2016akbc}), and use a random 70/30 split for training and testing. The ALTA dataset is a balanced set which contains 400 pairs of URLs that can refer to any entity -- person, organisation, location, etc. We used 300 pairs for training, and used the heldout test set of 100 pairs for testing. 

Our features are based on those proposed by the best performing systems in the respective shared tasks. In brief, the 8 features used for WePS include: unigram cosine similarity, $n$-gram cosine similarity,
% TJB: what is $n$?
named entity-based cosine similarity (obtained using Stanford CoreNLP NER), title unigram Jaccard Coefficient score, URL character 4-gram Jaccard coefficient, document-level cosine similarity over an average word-level word2vec representation, document length difference, and URL path length difference. We used the same set of features for the ALTA dataset, in addition to semantic similarity\footnote{\url{https://dandelion.eu/semantic-text/text-similarity-demo/}} and machine translation metric-based similarity\footnote{\url{https://github.com/jhclark/multeval}} (BLEU, METEOR and TER) between pairs of Bing search snippet text provided in the dataset (12 features in total).
%one-class SVM (OSVM) using $D_{p}$ only

Using distant supervision (\secref{sec:querying}) we got around 10\% of positive examples for WePS and around 25\% for ALTA. With $D_{p}$ as positive and $D_{u}$ as negative examples, we use biased SVM (``BSVM'') with different costs for positive and negative classes as a simple baseline. PU Learning baselines with $D_{p}$ and $D_{u}$ include: Spy-SVM with recommended parameters \cite{liu2002partially} and SPUL \cite{Elkan:2008:LCO:1401890.1401920}. All SVM-based models use a linear kernel. We also include an unsupervised approach based on  hierarchical clustering (referred to as ``HC'') which has been shown to perform well for the WePS dataset \cite{delgado2014data}. We set the number of clusters to 2, and use a brute-force technique to minimise overall error in assigning clusters to classes.
% We consider binary classification with $D_{p}$ as positive and $D_{u}$ as negative examples as a simple baseline. We use SVM and biased-SVM (BSVM) which has different costs for positive and negative classes for as binary classification. 
% On WePS dataset, using distant supervision, we got around 10\% positive examples, with a precision of $\sim80\%$.  On ALTA dataset, we obtained aroud 25\% %positive examples, with a precision of $\sim71\%$. Lesser recall of positive examples for WePS dataset is expected since the dataset consists of old URLs, which %may not be popular currently. Similarly, lesser precision of positive examples for ALTA dataset is due to social media URLs such as Twitter. 
%We compute average F-measure on test-set (Table 1) to compare the proposed setup with baseline approaches. 
Hyperparameters for the SVM ($C$) and biased-SVM ($C$ and class weights) were tuned using cross-validation. For our proposed method (``DP-SVM''), as described in \secref[s]{sec:querying} and \ref{sec:labelling}, we provide average scores across runs with $N$=5 for constructing random positive sets $S_{p}$. We present the micro-averaged F-score across five runs for the test set of the two datasets in \tabref{tab:results}. 

The distant supervision-based model based on \secref{sec:querying} (``BSVM'') performs better than the unsupervised approach (HC). SPUL is sensitive to the ratio of positive examples ($|D_{p}|$ vs.\ $|D_{u}|$), and performs better on the ALTA dataset than the WePS dataset. Overall, our proposed approach performs better the other competing methods. We evaluated the upper bound performance of a supervised SVM model built with manually annotated labels, and found it to be around 5\% greater than our proposed approach in absolute terms on both datasets.

%Since WePS dataset has rich features from webpage content, the performance improvement on it is higher using the proposed approach and Spy-SVM. ALTA has many URLs from social sources such as Twitter, which may not always have rich text features. 

%, though using a non-linear classifier finally could provide better performance, here we show only the relative gain of using distant supervision in conjunction with PU Learning
%

%%Supervised SVM model built on true labels of WePS gives a performance of 0.646 on test data, similarly supervised model on ALTA dataset provides a %performance of 0.303, from competition it is 0.658.
%
%\begin{small}
%\begin{center}
%\small \textbf{Table1: Micro-Average F-measure}
% \begin{tabular}{ | l|c|c|c|c|c|c|c| }
%    \hline
%Dataset &SVM & OSVM & Spy-SVM & BSVM & SPUL & $SVM^{P}$ & $BSVM^{P}$ \\ \hline
%  WePS & 0.298 & 0.454 & 0.362 & 0.452 & 0.364 & 0.504 & 0.545 \\ \hline
%    ALTA & 0.322 & 0.505 & 0.406 & 0.458 & 0.552 & 0.570 & 0.577 \\ \hline
%  \end{tabular}
%\end{center}
%\end{small}

%\begin{small}
%\begin{center}
%\small \textbf{Table1: Micro-Average F-measure}
% \begin{tabular}{ | l|c|c|c|c|c|c| }
%    \hline
%Dataset &Spy-SVM & OSVM & BSVM & SPUL  & $BSVM^{P}$ & HC \\ \hline
%  WePS & 0.362 & 0.454 & 0.452 &  0.364  & 0.539  & 0.475\\ \hline
%    ALTA & 0.406 & 0.505 & 0.458 & 0.552  & 0.622 & 0.479\\ \hline
%  \end{tabular}
%\end{center}
%\end{small}

%
%\begin{small}
%\begin{center}
%\small \textbf{Table1: Average F-measure}
% \begin{tabular}{ | l|c|c|c|c|c|c|c|}
%    \hline
%Dataset & SVM & BSVM &Spy-SVM& SPUL  & HC & D-SVM  \\ \hline
%  WePS &  0.298 & 0.536 &  0.631 & 0.364  &  0.408 & \textbf{0.635}\\ \hline
%    ALTA & 0.322  & 0.487 &0.504 & 0.552  &  0.481 & \textbf{0.607}\\ \hline
%  \end{tabular}
%\end{center}
%\end{small}



%\begin{small}
%\begin{center}
%\small \textbf{Table1: Micro-Average F-measure}
% \begin{tabular}{ | l|c|c|c|c|c|c|c|}
%    \hline
%Dataset & SVM & BSVM &Spy-SVM& SPUL  & HC & DP-SVM  \\ \hline
%  WePS &  0.467 & 0.472 &  0.63 & 0.516  &  0.408 & \textbf{0.653}\\ \hline
%    ALTA & 0.49 & 0.5 &0.54 & 0.587  &  0.481 & \textbf{0.608}\\ \hline
%  \end{tabular}
%\end{center}
%\end{small}


\begin{table}[t]
  \centering
\begin{small}
 \begin{tabular}{lcccccc}
    \toprule
Dataset &  BSVM &Spy-SVM& SPUL  & HC & DP-SVM  \\
 \midrule
  WePS &   0.472 &  0.630 & 0.516  &  0.408 & \textbf{0.653}\\
    ALTA &  0.500 &0.540 & 0.587  &  0.481 & \textbf{0.608}\\
% TJB: normalised to 3 decimal places, but @Shiva to check that the results are correct
 \bottomrule
  \end{tabular}
\end{small}
\vspace{-3ex}
\caption{Micro-averaged F-score}
\label{tab:results}
\end{table}
%\begin{small}
%\begin{center}
%\small \textbf{Table1: Micro-Average F-measure}
% \begin{tabular}{ | l|c|c|c|c|c|c|}
%    \hline
%Dataset &  BSVM &Spy& SPUL  & HC & DP-SVM & Sup \\ \hline
%  WePS &   0.472 &  0.63 & 0.516  &  0.408 & \textbf{0.653} &  0.408\\ \hline
%    ALTA &  0.5 &0.54 & 0.587  &  0.481 & \textbf{0.608} &  0.408\\ \hline
%  \end{tabular}
%\end{center}
%\end{small}


%Additionally, we also evaluate the proposed approach against commonly used K-Means in WePS systems \cite{delgado2014data}. We compute accuracy of after assigning the clusters to positive and negative classes. 
%
%\begin{small}
%\begin{center}
%\begin{table}[!htp]
%  \begin{tabular}{ |l|c|r|}
%    \hline
%Dataset &HC & Proposed \\ \hline
%  WePS & 50.3\% & 54.5\% \\ \hline
%    ALTA & 54\% & 60\%\\ \hline
%  \end{tabular}
%\caption{Accuracy}
%\end{table}
%\end{center}
%\end{small}


\section{Conclusions}

We have proposed an approach to determining whether two endpoint URLs refer to the same entity, with two key contributions: (a) the use of distant supervision; and (b) the application of PU Learning to the task. To the best of our knowledge, this is the first attempt to leverage distant supervision in conjunction with PU Learning. 
%for labeling positive examples, and to employ PU Learning to build a model. 

\bibliographystyle{abbrv}
\bibliography{WWW}  
\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
